<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>论文阅读笔记-GNN综述 | Fighting</title><meta name="keywords" content="Paper Note,GNN"><meta name="author" content="Yxmlzl"><meta name="copyright" content="Yxmlzl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文阅读笔记-GNN综述主要介绍了GNN以及它在各个领域的应用 2D NATURAL IMAGESImage ClassificationMulti-Label ClassificationML-GCN:builds a directed graph on the basis of label space, where each node stands for a object label (wo">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读笔记-GNN综述">
<meta property="og:url" content="https://lzl2040.github.io/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="Fighting">
<meta property="og:description" content="论文阅读笔记-GNN综述主要介绍了GNN以及它在各个领域的应用 2D NATURAL IMAGESImage ClassificationMulti-Label ClassificationML-GCN:builds a directed graph on the basis of label space, where each node stands for a object label (wo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-10-07T05:06:27.000Z">
<meta property="article:modified_time" content="2022-10-07T16:04:17.631Z">
<meta property="article:author" content="Yxmlzl">
<meta property="article:tag" content="Paper Note">
<meta property="article:tag" content="GNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lzl2040.github.io/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读笔记-GNN综述',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-08 00:04:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Fighting" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/pictures/"><i class="fa-fw fas fa-images"></i><span> 图片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Fighting</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/pictures/"><i class="fa-fw fas fa-images"></i><span> 图片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读笔记-GNN综述</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-07T05:06:27.000Z" title="发表于 2022-10-07 13:06:27">2022-10-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-07T16:04:17.631Z" title="更新于 2022-10-08 00:04:17">2022-10-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读笔记-GNN综述"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="论文阅读笔记-GNN综述"><a href="#论文阅读笔记-GNN综述" class="headerlink" title="论文阅读笔记-GNN综述"></a>论文阅读笔记-GNN综述</h1><p>主要介绍了GNN以及它在各个领域的应用</p>
<h2 id="2D-NATURAL-IMAGES"><a href="#2D-NATURAL-IMAGES" class="headerlink" title="2D NATURAL IMAGES"></a>2D NATURAL IMAGES</h2><h3 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h3><h4 id="Multi-Label-Classification"><a href="#Multi-Label-Classification" class="headerlink" title="Multi-Label Classification"></a>Multi-Label Classification</h4><p><strong>ML-GCN</strong>:builds a directed graph on the basis of label space, where each node stands for a object label (word embeddings) and their connections model the inter-dependencies of different labels.</p>
<p><strong>attention-driven GCN</strong>:model the label dependencies via more elaborate GNN architectures</p>
<p><strong>hypergraph neural networks</strong>:model the label dependencies via more elaborate GNN architectures</p>
<h4 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h4><table>
<thead>
<tr>
<th>论文名称</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Few-shot learning with graph neural networks</td>
<td>ICLR,2018</td>
<td>formulate FSL as a supervised interpolation problem on a densely-connected graph, where the vertices stand for images in the collection and the adjacency is learnable with trainable similarity kernels.</td>
</tr>
<tr>
<td>Learning to propagate labels: Transductive propagation network for few-shot learning</td>
<td>ICLR,2019</td>
<td>constructs graphs on the top of embedding space to fully exploit the manifold structure of the novel classes.Label information is propagated from the support set to the query set based on the constructed graphs</td>
</tr>
<tr>
<td>dge-labeling graph neural network for few-shot learning</td>
<td>CVPR,2019</td>
<td>propose a edge-labeling GNN framework that learns to predict edge labels, explicitly constraining the intra- and inter-class similarities.</td>
</tr>
<tr>
<td>Learning from the past: Continual meta-learning via bayesian graph modeling</td>
<td>AAAI,2020</td>
<td>formulate meta-learning-based FSL as continual learning of a sequence of tasks and resort to Bayesian GNN to capture the intra- and inter-task correlations.</td>
</tr>
<tr>
<td>Dpgn: Distribution propagation graph network for few-shot learning</td>
<td>CVPR,2020</td>
<td>devise a dual complete graph network to model both distribution- and instance-level relations.</td>
</tr>
<tr>
<td>Hierarchical graph neural networks for few-shot learning</td>
<td>TCSVT,2021</td>
<td>exploit the hierarchical relationships among graph nodes via the bottom-up and top-down reasoning modules.</td>
</tr>
<tr>
<td>Hybrid graph neural networks for few-shot learning</td>
<td>AAAI,2022</td>
<td>introduce an instance GNN and a prototype GNN as feature embedding task adaptation modules for quickly adapting learned features to new tasks.</td>
</tr>
</tbody></table>
<h4 id="Zero-Shot-Learning-ZSL"><a href="#Zero-Shot-Learning-ZSL" class="headerlink" title="Zero-Shot Learning (ZSL)"></a>Zero-Shot Learning (ZSL)</h4><table>
<thead>
<tr>
<th>论文名称</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Rethinking knowledge graph propagation for zero-shot learning</td>
<td>CVPR,2019</td>
<td>propose a Dense Graph Propagation (DGP) module to exploit the hierarchical structure of knowledge graph.It consists of two phases to iteratively propagate knowledge between a node and its ancestors and descendants.</td>
</tr>
<tr>
<td>Region graph embedding network for zero-shot learning</td>
<td>ECCV,2020</td>
<td>represent each input image as a region graph, where each node stands for an attended region in the image and the edges are appearance similarities among these region nodes.</td>
</tr>
<tr>
<td>Attribute propagation network for graph zero-shot learning</td>
<td>AAAI,2020</td>
<td>generates and updates attribute vectors with an attribute propagation network for optimizing the attribute space</td>
</tr>
<tr>
<td>Isometric propagation network for generalized zero-shot learning</td>
<td>ICLR,2021</td>
<td>introduce the visual and semantic prototype propagation on auto-generated graphs to enhance the inter-class relations and align the corresponding classwise dependencies in visual and semantic space</td>
</tr>
<tr>
<td>Learning graph embeddings for open world compositional zero-shot learning</td>
<td>TPAMI, 2022</td>
<td>introducing a Compositional Cosine Graph Embedding (Co-CGE) model to learn the relationship between primitives and compositions through a GCN.They quantitatively measure the feasibility scores of a state-object composition and incorporate the computed scores into CoCGE in two ways</td>
</tr>
<tr>
<td>Gndan: Graph navigated dual attention network for zero-shot learning</td>
<td>IEEE TNNLS, 2022</td>
<td>resort to GAT for exploiting the appearance relations between local regions and the cooperation between local and global features.</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h4 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h4><table>
<thead>
<tr>
<th>论文名称</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Gcan: Graph convolutional adversarial network for unsupervised domain adaptation</td>
<td>CVPR，2019</td>
<td>propose a Graph Convolutional Adversarial Network (GCAN) for DA, where a GCN is developed on top of densely-connected instance graphs to encode data structure information.</td>
</tr>
<tr>
<td>Heterogeneous graph attention network for unsupervised multiple-target domain adaptation</td>
<td>IEEE TPAMI, 2020</td>
<td>build a heterogeneous relation graph and introduce GAT to propagate the semantic information and generate reliable pseudo-labels.</td>
</tr>
<tr>
<td>Curriculum graph co-teaching for multi-target domain adaptation</td>
<td>CVPR,2021</td>
<td>introduce a GCN to aggregate information from different domains along with a co-teaching and curriculum learning strategy to achieve progressive adaptation.</td>
</tr>
<tr>
<td>Progressive graph learning for open-set domain adaptation</td>
<td>ICML,2020</td>
<td>study the problem of open-set DA via a progressive graph learning framework to select pseudo-labels and thus avoid the negative transfer.</td>
</tr>
<tr>
<td>Prototype-matching graph network for heterogeneous domain adaptation</td>
<td>ACMMM 2020</td>
<td>attain cross-domain prototype alignment based on features learned from different stages of GNNs.</td>
</tr>
<tr>
<td>Learning to combine: Knowledge aggregation for multi-source domain adaptation</td>
<td>ECCV. Springer, 2020.</td>
<td>introduce a knowledge graph based on the prototypes of different domains to perform information propagation among semantically adjacent representations.</td>
</tr>
<tr>
<td>Compound domain generalization via meta-knowledge encoding</td>
<td>CVPR,2022</td>
<td>build global prototypical relation graphs and introduce a graph self-attention mechanism</td>
</tr>
</tbody></table>
<h4 id="当前工作重点"><a href="#当前工作重点" class="headerlink" title="当前工作重点"></a>当前工作重点</h4><p>Current work focuses on extracting adhoc knowledge graphs from the data for a certain task, which is heuristic and relies on the human prior</p>
<h4 id="未来的方向"><a href="#未来的方向" class="headerlink" title="未来的方向"></a>未来的方向</h4><p>（1）develop general and automatic graph construction procedures,</p>
<p>（2）enhance the interactions between abstract graph structures and task-specific classifiers</p>
<p>（3）excavate more fine-grained building blocks (node and edge) to increase the capability of constructed graphs.</p>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><table>
<thead>
<tr>
<th>论文名称</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Reasoning-rcnn: Unifying adaptive global reasoning into large-scale object detection</td>
<td>CVPR,2019</td>
<td>presents an adaptive global reasoning network for large-scale object detection by incorporating commonsense knowledge (category-wise knowledge graph) and propagating visual information globally</td>
</tr>
<tr>
<td>Spatial-aware graph relation network for large-scale object detection</td>
<td>CVPR,2019</td>
<td>adaptively discover semantic and spatial relationships without requiring prior handcrafted linguistic knowledge</td>
</tr>
<tr>
<td>Relation networks for object detection</td>
<td>CVPR,2018</td>
<td>introduces an adapted attention module to detection head networks, explicitly learning information between objects through encoding the longrange dependencies.</td>
</tr>
<tr>
<td>Relationnet++: Bridging visual representations for object detection via transformer decoder</td>
<td>NeurIPS,2020</td>
<td>presents a selfattention-based decoder module to embrace the strengths of different object/part representations within a single detection framework.</td>
</tr>
<tr>
<td>Gar: Graph assisted reasoning for object detection</td>
<td>WACV,2020</td>
<td>introduce a heterogeneous graph to jointly model object-object and object-scene relations.</td>
</tr>
<tr>
<td>Graphfpn: Graph feature pyramid network for object detection</td>
<td>ICCV,2021</td>
<td>propose a graph feature pyramid network (GraphFPN), which explores the contextual and hierarchical structures of an input image based on a superpixel hierarchy</td>
</tr>
<tr>
<td>Relation matters: Foreground-aware graph-based relational reasoning for domain adaptive object detection</td>
<td>IEEE TPAMI,2022</td>
<td>first builds intra- and inter-domain relation graphs in virtue of cyclic between-domain consistency without any prior knowledge about the target distribution.</td>
</tr>
<tr>
<td>Sigma: Semantic-complete graph matching for domain adaptive object detection</td>
<td>ICCV,2021</td>
<td>formulates DAOD as a graph matching problem by establishing cross-image graphs to model classconditional distributions on both domains</td>
</tr>
<tr>
<td>Semantic relation reasoning for shot-stable few-shot object detection</td>
<td>CVPR,2022</td>
<td>introduces a semantic relation reasoning module to integrate semantic information between base and novel classes for novel object detection</td>
</tr>
</tbody></table>
<p>说明：domain adaptive object detection (DAOD)</p>
<h4 id="当前的工作重点"><a href="#当前的工作重点" class="headerlink" title="当前的工作重点"></a>当前的工作重点</h4><p>exploit between-object, cross-scale or cross-domain relationships, as well as relationships between base and novel classes</p>
<h4 id="未来的方向-1"><a href="#未来的方向-1" class="headerlink" title="未来的方向"></a>未来的方向</h4><p>（1）design better region-to-node feature mapping methods,</p>
<p>（2）incorporate Transformer (or pure GNN) encoders to improve the expressive power of initial node features</p>
<p>（3）directly perform reasoning in the original feature space to better preserve the intrinsic structure of images.</p>
<h3 id="Image-Segmentation"><a href="#Image-Segmentation" class="headerlink" title="Image Segmentation"></a>Image Segmentation</h3><h4 id="一般的分割"><a href="#一般的分割" class="headerlink" title="一般的分割"></a>一般的分割</h4><table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Dual graph convolutional network for semantic segmentation</td>
<td>BMVC,2019</td>
<td>targets on modeling the global context of input features via a dual GCN framework where a coordinate space GCN models spatial relationships between pixels in the image, and a feature space GCN models dependencies along the channel dimensions of the network’s feature map.</td>
</tr>
<tr>
<td>Graph-based global reasoning networks</td>
<td>CVPR,2019</td>
<td>design the global reasoning unit by projecting features that are globally aggregated in coordinate space to node domain and performing relational reasoning in a fullyconnected graph.</td>
</tr>
<tr>
<td>Dynamic graph message passing networks</td>
<td>CVPR,2020</td>
<td>dynamically samples the neighborhood of a node and then predicts the node dependencies, filter weights, and affinity matrix to attain information propagation</td>
</tr>
<tr>
<td>Representative graph neural network</td>
<td>ECCV,2020</td>
<td>propose to dynamically sample some representative nodes for relational modeling.</td>
</tr>
<tr>
<td>Spatial pyramid based graph reasoning for semantic segmentation</td>
<td>CVPR,2020</td>
<td>propose an improved Laplacian formulation that enables graph reasoning in the original feature space, fully exploiting the contextual relations at different feature scales.</td>
</tr>
<tr>
<td>Class-wise dynamic graph convolution for semantic segmentation</td>
<td>ECCV,2020</td>
<td>introduce a classwise dynamic graph convolution module to conduct graph reasoning over the pixels that belong to the same class</td>
</tr>
<tr>
<td>Bidirectional graph reasoning network for panoptic segmentation</td>
<td>CVPR,2020</td>
<td>design a bidirectional graph reasoning network to bridge the things branch and the stuff branch for panoptic segmentation.</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h4 id="One-Shot-Semantic-Segmentation"><a href="#One-Shot-Semantic-Segmentation" class="headerlink" title="One-Shot Semantic Segmentation"></a>One-Shot Semantic Segmentation</h4><table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Pyramid graph networks with connection attentions for region-based oneshot semantic segmentation</td>
<td>ICCV,2019</td>
<td>introduce a pyramid graph attention module to model the connection between query and support feature maps</td>
</tr>
</tbody></table>
<h4 id="Few-Shot-Semantic-Segmentation"><a href="#Few-Shot-Semantic-Segmentation" class="headerlink" title="Few-Shot Semantic Segmentation"></a>Few-Shot Semantic Segmentation</h4><table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Scale-aware graph neural network for few-shot semantic segmentation</td>
<td>CVPR,2021</td>
<td>propose a scale-aware GNN to perform crossscale relational reasoning among support-query images. A self-node collaboration mechanism is introduced to perceive different resolutions of the same object.</td>
</tr>
</tbody></table>
<h4 id="Weakly-Supervised-Semantic-Segmentation"><a href="#Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Weakly Supervised Semantic Segmentation"></a>Weakly Supervised Semantic Segmentation</h4><table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Affinity attention graph neural network for weakly supervised semantic segmentation</td>
<td>IEEE,TPAMI 2021</td>
<td>an image will first be converted to a weighted graph via an affinity CNN network, and then an affinity attention layer is devised to obtain long-range interactions from the constructed graph and propagate semantic information to the unlabeled pixels</td>
</tr>
</tbody></table>
<h4 id="当前的工作重点-1"><a href="#当前的工作重点-1" class="headerlink" title="当前的工作重点"></a>当前的工作重点</h4><p>explore contextual information in the localor global-level with pyramid pooling, dilated convolutions, or the self-attention mechanism</p>
<h3 id="Scene-Graph-Generation-SGG"><a href="#Scene-Graph-Generation-SGG" class="headerlink" title="Scene Graph Generation (SGG)"></a>Scene Graph Generation (SGG)</h3><p>任务概述：检测图像中的对象对及其关系以生成可视化的场景图的任务，它提供了对视觉场景的高级理解，而不是孤立地处理单个对象</p>
<table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td>Factorizable net: an efficient subgraph-based framework for scene graph generation</td>
<td>ECCV,2018</td>
<td>a subgraph-based approach (each subgraph is regarded as a node), has a spatially weighted message passing structure to refine the features of objects and subgroups by passing messages among them with attention-like schemes</td>
</tr>
<tr>
<td>Graph r-cnn for scene graph generation</td>
<td>ECCV,2018</td>
<td>first obtain a sparse candidate graph by pruning the densely-connected graph generated from RPN via a relation proposal network, then an attentional GCN is introduced to aggregate contextual information and update node features and edge relationships</td>
</tr>
<tr>
<td>Attentive relational networks for mapping images to scene graphs</td>
<td>CVPR,2019</td>
<td>propose attentive relational networks, which first transform label word embeddings and visual features into a shared semantic space, and then rely on GAT to perform feature aggregation for final relation inference</td>
</tr>
<tr>
<td>Bipartite graph network with adaptive message passing for unbiased scene graph generation</td>
<td>CVPR,2021</td>
<td>introduce bipartite GNN to estimate and propagate relation confidence in a multi-stage manner.</td>
</tr>
<tr>
<td>Energy-based learning for scene graph generation</td>
<td>CVPR,2021</td>
<td>propose an energybased framework, which depends on graph message passing algorithm for computing the energy of configurations.</td>
</tr>
</tbody></table>
<h2 id="VIDEO-UNDERSTANDING"><a href="#VIDEO-UNDERSTANDING" class="headerlink" title="VIDEO UNDERSTANDING"></a>VIDEO UNDERSTANDING</h2><h3 id="Video-Action-Recognition"><a href="#Video-Action-Recognition" class="headerlink" title="Video Action Recognition"></a>Video Action Recognition</h3><p>任务介绍：视频人体动作识别是视频处理和理解的基本任务之一，其目的是识别和分类RGB/深度视频或骨架数据中的人体动作。</p>
<h4 id="Action-Recognition"><a href="#Action-Recognition" class="headerlink" title="Action Recognition"></a>Action Recognition</h4><table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td>propose to capture the long-range temporal contexts via graph-based reasoning over human-object and object-object relationships</td>
</tr>
<tr>
<td></td>
<td></td>
<td>construct actor-centric object-level graph and applying GCNs to capture the contexts among objects in a actor-centric way.A relation-level graph is built to inference the contexts in relation nodes</td>
</tr>
<tr>
<td></td>
<td></td>
<td>propose multi-scale reasoning in the temporal graph of a video, in which each node is a frame in the video, and the pairwise relations between nodes are represented as a learnable adjacent matrix</td>
</tr>
<tr>
<td></td>
<td></td>
<td>extend the GCN-based relation modeling to zero-shot action recognition and leverage knowledge graphs to model the relations among actions and attributes jointly</td>
</tr>
<tr>
<td></td>
<td></td>
<td>introduce a graph-based high-order relation modeling method for long-term action recognition.</td>
</tr>
</tbody></table>
<h4 id="Skeleton-Based-Action-Recognition"><a href="#Skeleton-Based-Action-Recognition" class="headerlink" title="Skeleton-Based Action Recognition."></a>Skeleton-Based Action Recognition.</h4><table>
<thead>
<tr>
<th>论文题目</th>
<th>来源</th>
<th>主要思想</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td>propose a STGCN network first connects joints in a frame according to the natural connectivity in the human body and then connects the same joints in two consecutive frames to maintain temporal information.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>introduce a fully-connected graph with learnable edge weights between joints and a data-dependent graph learned from the input skeleton.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>connect physically-apart skeleton joints to captures the patterns of collaborative moving joints</td>
</tr>
<tr>
<td></td>
<td></td>
<td>improves the joints’ connection in a single frame by adding edges between limbs and head.it uses GCNs to capture joints’ relations in single frames and adopt the LSTM to capture the temporal dynamics.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>introduce to maintain edge features and learn both node and edge feature representations via directed graph convolution.</td>
</tr>
<tr>
<td></td>
<td></td>
<td>first construct multiple dilated windows over temporal dimension.Then separately utilize GCNs on multiple graphs with different scales.Finally aggregate the results of GCNs on all the graphs in multiple windows to capture multi-scale and long-range dependencies.</td>
</tr>
</tbody></table>
<h3 id="Temporal-Action-Localization"><a href="#Temporal-Action-Localization" class="headerlink" title="Temporal Action Localization"></a>Temporal Action Localization</h3></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Yxmlzl</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lzl2040.github.io/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/">https://lzl2040.github.io/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lzl2040.github.io" target="_blank">Fighting</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Paper-Note/">Paper Note</a><a class="post-meta__tags" href="/tags/GNN/">GNN</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Graph-Based-Global-Reasoning-Networks/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">论文阅读-Graph-Based Global Reasoning Networks</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Masked-Autoencoders-Are-Scalable-Vision-Learners/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">论文阅读-Masked Autoencoders Are Scalable Vision Learners</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Graph-Based-Global-Reasoning-Networks/" title="论文阅读-Graph-Based Global Reasoning Networks"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-13</div><div class="title">论文阅读-Graph-Based Global Reasoning Networks</div></div></a></div><div><a href="/2022/09/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Prototype-Mixture-Models-for-Few-shot-Semantic-Segmentation/" title="论文阅读-Prototype Mixture Models for Few-shot Semantic Segmentation"><img class="cover" src="https://s1.ax1x.com/2022/09/16/vzcrB6.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-15</div><div class="title">论文阅读-Prototype Mixture Models for Few-shot Semantic Segmentation</div></div></a></div><div><a href="/2022/09/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Delving-Deep-into-Many-to-many-Attention-for-Few-shot-Video-Object-Segmentation/" title="论文阅读-Delving Deep into Many-to-many Attention for Few-shot Video Object Segmentation Segmentation"><img class="cover" src="https://s1.ax1x.com/2022/09/20/xCXUxJ.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-16</div><div class="title">论文阅读-Delving Deep into Many-to-many Attention for Few-shot Video Object Segmentation Segmentation</div></div></a></div><div><a href="/2022/09/18/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E8%AE%BA%E6%96%87%E5%B8%B8%E7%94%A8%E9%AB%98%E7%BA%A7%E8%AF%8D%E6%B1%87-%E5%8F%A5%E5%BC%8F/" title="论文笔记-论文常用高级词汇/句式"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-18</div><div class="title">论文笔记-论文常用高级词汇/句式</div></div></a></div><div><a href="/2022/09/18/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Modular-Interactive-Video-Object-Segmentation-Interaction-to-Mask-Propagation-and-Difference-Aware-Fusion/" title="论文阅读-Modular Interactive Video Object Segmentation:Interaction-to-Mask,Propagation and Difference-Aware Fusion"><img class="cover" src="https://s1.ax1x.com/2022/09/20/xCX8aV.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-18</div><div class="title">论文阅读-Modular Interactive Video Object Segmentation:Interaction-to-Mask,Propagation and Difference-Aware Fusion</div></div></a></div><div><a href="/2022/09/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PANet-Few-Shot-Image-Semantic-Segmentation-with-Prototype-Alignment/" title="论文阅读-PANet:Few-Shot Image Semantic Segmentation with Prototype Alignment"><img class="cover" src="https://s1.ax1x.com/2022/09/20/xP7ste.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-20</div><div class="title">论文阅读-PANet:Few-Shot Image Semantic Segmentation with Prototype Alignment</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yxmlzl</div><div class="author-info__description">逆袭!</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lzl2040"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lzl2040" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=jL68uLy9u725tLrM-f2i7_Ph" target="_blank" title="Email"><i class="fas fa-envelope-square"></i></a><a class="social-icon" href="https://user.qzone.qq.com/2040171586" target="_blank" title="QQ"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">论文阅读笔记-GNN综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2D-NATURAL-IMAGES"><span class="toc-number">1.1.</span> <span class="toc-text">2D NATURAL IMAGES</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Image-Classification"><span class="toc-number">1.1.1.</span> <span class="toc-text">Image Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-Label-Classification"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Multi-Label Classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Few-Shot-Learning"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">Few-Shot Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Zero-Shot-Learning-ZSL"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">Zero-Shot Learning (ZSL)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transfer-Learning"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">Transfer Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E5%B7%A5%E4%BD%9C%E9%87%8D%E7%82%B9"><span class="toc-number">1.1.1.5.</span> <span class="toc-text">当前工作重点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E7%9A%84%E6%96%B9%E5%90%91"><span class="toc-number">1.1.1.6.</span> <span class="toc-text">未来的方向</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object-Detection"><span class="toc-number">1.1.2.</span> <span class="toc-text">Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E5%B7%A5%E4%BD%9C%E9%87%8D%E7%82%B9"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">当前的工作重点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E7%9A%84%E6%96%B9%E5%90%91-1"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">未来的方向</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Image-Segmentation"><span class="toc-number">1.1.3.</span> <span class="toc-text">Image Segmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E8%88%AC%E7%9A%84%E5%88%86%E5%89%B2"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">一般的分割</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#One-Shot-Semantic-Segmentation"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">One-Shot Semantic Segmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Few-Shot-Semantic-Segmentation"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">Few-Shot Semantic Segmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Weakly-Supervised-Semantic-Segmentation"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">Weakly Supervised Semantic Segmentation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E5%B7%A5%E4%BD%9C%E9%87%8D%E7%82%B9-1"><span class="toc-number">1.1.3.5.</span> <span class="toc-text">当前的工作重点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scene-Graph-Generation-SGG"><span class="toc-number">1.1.4.</span> <span class="toc-text">Scene Graph Generation (SGG)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VIDEO-UNDERSTANDING"><span class="toc-number">1.2.</span> <span class="toc-text">VIDEO UNDERSTANDING</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Video-Action-Recognition"><span class="toc-number">1.2.1.</span> <span class="toc-text">Video Action Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Action-Recognition"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Action Recognition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skeleton-Based-Action-Recognition"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Skeleton-Based Action Recognition.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Temporal-Action-Localization"><span class="toc-number">1.2.2.</span> <span class="toc-text">Temporal Action Localization</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/10/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-CLIP4Clip-An-Empirical-Study-of-CLIP-for-End-to-End-Video-Clip-Retrieval/" title="论文阅读-CLIP4Clip An Empirical Study of CLIP for End to End Video Clip Retrieval"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读-CLIP4Clip An Empirical Study of CLIP for End to End Video Clip Retrieval"/></a><div class="content"><a class="title" href="/2022/10/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-CLIP4Clip-An-Empirical-Study-of-CLIP-for-End-to-End-Video-Clip-Retrieval/" title="论文阅读-CLIP4Clip An Empirical Study of CLIP for End to End Video Clip Retrieval">论文阅读-CLIP4Clip An Empirical Study of CLIP for End to End Video Clip Retrieval</a><time datetime="2022-10-15T01:05:11.000Z" title="发表于 2022-10-15 09:05:11">2022-10-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Graph-Based-Global-Reasoning-Networks/" title="论文阅读-Graph-Based Global Reasoning Networks"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读-Graph-Based Global Reasoning Networks"/></a><div class="content"><a class="title" href="/2022/10/13/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Graph-Based-Global-Reasoning-Networks/" title="论文阅读-Graph-Based Global Reasoning Networks">论文阅读-Graph-Based Global Reasoning Networks</a><time datetime="2022-10-13T07:35:08.000Z" title="发表于 2022-10-13 15:35:08">2022-10-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/" title="论文阅读笔记-GNN综述"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读笔记-GNN综述"/></a><div class="content"><a class="title" href="/2022/10/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-GNN%E7%BB%BC%E8%BF%B0/" title="论文阅读笔记-GNN综述">论文阅读笔记-GNN综述</a><time datetime="2022-10-07T05:06:27.000Z" title="发表于 2022-10-07 13:06:27">2022-10-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Masked-Autoencoders-Are-Scalable-Vision-Learners/" title="论文阅读-Masked Autoencoders Are Scalable Vision Learners"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读-Masked Autoencoders Are Scalable Vision Learners"/></a><div class="content"><a class="title" href="/2022/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Masked-Autoencoders-Are-Scalable-Vision-Learners/" title="论文阅读-Masked Autoencoders Are Scalable Vision Learners">论文阅读-Masked Autoencoders Are Scalable Vision Learners</a><time datetime="2022-09-28T08:23:33.000Z" title="发表于 2022-09-28 16:23:33">2022-09-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Learning%20Position%20and%20Target%20Consistency%20for%20Memory-based%20Video%20Object/" title="论文阅读-Learning Position and Target Consistency for Memory-based Video Object Segmentation"><img src="https://s1.ax1x.com/2022/09/23/xklpjO.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读-Learning Position and Target Consistency for Memory-based Video Object Segmentation"/></a><div class="content"><a class="title" href="/2022/09/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Learning%20Position%20and%20Target%20Consistency%20for%20Memory-based%20Video%20Object/" title="论文阅读-Learning Position and Target Consistency for Memory-based Video Object Segmentation">论文阅读-Learning Position and Target Consistency for Memory-based Video Object Segmentation</a><time datetime="2022-09-22T08:35:28.000Z" title="发表于 2022-09-22 16:35:28">2022-09-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Yxmlzl</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'Cx0xN2xvx8v9196mnBEfe4ES-gzGzoHsz',
      appKey: 't6sfkpiURDPNb8wrz8omD4X1',
      avatar: 'monsterid',
      serverURLs: 'https://cx0xn2xv.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>